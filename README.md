# Kaggle_Titanic
Just trying out methods.

The python file follows the guided tutorial given on Kaggle.com, with a little more messing around by myself. The tutorial
uses scikit-learn random forest. The best prediction was arond .76.

The R file is my own code, taken from a homework problem, and applied to the Titanic dataset. It uses logistic regression, fitted
using a penalized likelihood. The MLE of the likelihood is found using the EM algoithm, and iterative reweighted least squares (IRLS)
to maximize. The best prediction was about 7.5. 

Overall, my best prediction was from the random forest package in R, at 7.8. 

Will probably get around to testing using SVM and some other models in the future. 
